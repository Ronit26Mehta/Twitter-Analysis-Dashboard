{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Dataset:\n",
      "Shape: (33, 9)\n",
      "\n",
      "Users Dataset:\n",
      "Shape: (27, 6)\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'can', 'until', \"wouldn't\", 'being', \"i'd\", 'they', 'mightn', 'these', \"they're\", \"you're\", 'me', 'herself', 'to', 'you', 'whom', 'themselves', 'after', \"we're\", 'each', 'him', 'his', \"mightn't\", 'once', \"they've\", 'did', \"don't\", \"i'll\", 'i', 'no', \"that'll\", 'here', 'mustn', 'in', 'who', 'through', 'against', 'your', 'other', 'didn', 'during', 'how', 'there', 'such', 'their', 'have', 'off', \"isn't\", 'shouldn', 'but', 'd', 'haven', \"should've\", \"you'll\", \"it'll\", \"shan't\", 'been', 'hadn', 'that', 'all', 'had', 'more', 've', 's', 'doing', 'above', 'theirs', 'which', \"wasn't\", 'now', 'be', 'below', 'hers', \"he'll\", 'most', \"didn't\", \"i'm\", \"hasn't\", 'she', 'about', 'wouldn', 'he', 'is', \"i've\", 'll', 'aren', 'because', 'on', 'then', \"couldn't\", 'wasn', 'needn', \"needn't\", 'few', 'under', 'between', 'over', 'doesn', 'out', 'isn', 'where', 'hasn', 'weren', 'down', 'her', 'ain', 'yours', 'my', \"mustn't\", \"she'll\", 'those', \"doesn't\", 'from', 'y', 'only', 'both', 'ourselves', 'for', \"it'd\", 'nor', 'than', \"you've\", 'himself', 'couldn', \"they'd\", \"we'll\", 'further', 'any', 'an', 'very', 'or', 'not', \"it's\", 'were', 'what', 'again', 'so', 'has', \"she'd\", \"won't\", 'should', 't', 'this', \"hadn't\", 'shan', 'just', 'myself', 'ma', 'when', 'into', 'won', 'it', 'at', 'the', 'was', 'too', 'o', 'a', 'same', 're', 'them', \"we've\", 'does', 'our', 'am', 'if', 'having', \"they'll\", 'and', 'ours', 'own', \"shouldn't\", 'its', 'don', \"haven't\", 'will', \"you'd\", 'itself', 'yourselves', 'are', \"he'd\", \"he's\", 'some', 'by', 'while', 'yourself', 'we', 'as', \"weren't\", 'of', \"we'd\", 'with', 'up', 'do', \"she's\", \"aren't\", 'before', 'why', 'm'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     66\u001b[39m vectorizer = TfidfVectorizer(max_features=\u001b[32m5\u001b[39m, stop_words=stop_words)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m tfidf_matrix = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m feature_names = vectorizer.get_feature_names_out()\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Create a column with top keywords for each tweet\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micro-finance\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micro-finance\\venv\\Lib\\site-packages\\sklearn\\base.py:1382\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1378\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1379\u001b[39m )\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micro-finance\\venv\\Lib\\site-packages\\sklearn\\base.py:436\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    429\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\micro-finance\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'can', 'until', \"wouldn't\", 'being', \"i'd\", 'they', 'mightn', 'these', \"they're\", \"you're\", 'me', 'herself', 'to', 'you', 'whom', 'themselves', 'after', \"we're\", 'each', 'him', 'his', \"mightn't\", 'once', \"they've\", 'did', \"don't\", \"i'll\", 'i', 'no', \"that'll\", 'here', 'mustn', 'in', 'who', 'through', 'against', 'your', 'other', 'didn', 'during', 'how', 'there', 'such', 'their', 'have', 'off', \"isn't\", 'shouldn', 'but', 'd', 'haven', \"should've\", \"you'll\", \"it'll\", \"shan't\", 'been', 'hadn', 'that', 'all', 'had', 'more', 've', 's', 'doing', 'above', 'theirs', 'which', \"wasn't\", 'now', 'be', 'below', 'hers', \"he'll\", 'most', \"didn't\", \"i'm\", \"hasn't\", 'she', 'about', 'wouldn', 'he', 'is', \"i've\", 'll', 'aren', 'because', 'on', 'then', \"couldn't\", 'wasn', 'needn', \"needn't\", 'few', 'under', 'between', 'over', 'doesn', 'out', 'isn', 'where', 'hasn', 'weren', 'down', 'her', 'ain', 'yours', 'my', \"mustn't\", \"she'll\", 'those', \"doesn't\", 'from', 'y', 'only', 'both', 'ourselves', 'for', \"it'd\", 'nor', 'than', \"you've\", 'himself', 'couldn', \"they'd\", \"we'll\", 'further', 'any', 'an', 'very', 'or', 'not', \"it's\", 'were', 'what', 'again', 'so', 'has', \"she'd\", \"won't\", 'should', 't', 'this', \"hadn't\", 'shan', 'just', 'myself', 'ma', 'when', 'into', 'won', 'it', 'at', 'the', 'was', 'too', 'o', 'a', 'same', 're', 'them', \"we've\", 'does', 'our', 'am', 'if', 'having', \"they'll\", 'and', 'ours', 'own', \"shouldn't\", 'its', 'don', \"haven't\", 'will', \"you'd\", 'itself', 'yourselves', 'are', \"he'd\", \"he's\", 'some', 'by', 'while', 'yourself', 'we', 'as', \"weren't\", 'of', \"we'd\", 'with', 'up', 'do', \"she's\", \"aren't\", 'before', 'why', 'm'} instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import HeatMap, TimestampedGeoJson\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import geocoder\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary nltk resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Tweets Dataset:\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(\"\\nUsers Dataset:\")\n",
    "print(f\"Shape: {users_df.shape}\")\n",
    "\n",
    "# Step 1: Sort data based on timestamps\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
    "tweets_df = tweets_df.sort_values(by='timestamp')\n",
    "\n",
    "# Step 2: Detect languages using TextBlob and add a language code column\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            return TextBlob(text).detect_language()\n",
    "        except:\n",
    "            return 'en'  # Default to English if detection fails\n",
    "    return 'en'  # Default for non-string values\n",
    "\n",
    "tweets_df['language_code'] = tweets_df['text'].apply(detect_language)\n",
    "\n",
    "# Step 3: Translate non-English text to English\n",
    "def translate_to_english(row):\n",
    "    if row['language_code'] != 'en' and isinstance(row['text'], str):\n",
    "        try:\n",
    "            return str(TextBlob(row['text']).translate(to='en'))\n",
    "        except:\n",
    "            return row['text']  # Return original if translation fails\n",
    "    return row['text']  # Return original for English or non-string values\n",
    "\n",
    "tweets_df['translated_text'] = tweets_df.apply(translate_to_english, axis=1)\n",
    "\n",
    "# Use translated_text for further analysis\n",
    "analysis_text = tweets_df['translated_text'].fillna('')\n",
    "\n",
    "# Step 4: Extract keywords using TF-IDF\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(max_features=5, stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(analysis_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a column with top keywords for each tweet\n",
    "def get_top_keywords(tfidf_row, feature_names):\n",
    "    indices = np.argsort(tfidf_row)[::-1]\n",
    "    top_keywords = [(feature_names[i], tfidf_row[i]) for i in indices if tfidf_row[i] > 0]\n",
    "    return top_keywords\n",
    "\n",
    "tweets_df['keywords'] = [get_top_keywords(tfidf_matrix[i].toarray()[0], feature_names) \n",
    "                        for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Step 5: Extract sentiment for each keyword and the entire sentence\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "# Sentiment for each keyword and entire sentence\n",
    "tweets_df['keyword_sentiments'] = tweets_df['keywords'].apply(\n",
    "    lambda kw_list: [(kw, score, get_sentiment(kw)) for kw, score in kw_list] if isinstance(kw_list, list) else []\n",
    ")\n",
    "tweets_df['sentence_sentiment'] = tweets_df['translated_text'].apply(get_sentiment)\n",
    "\n",
    "# Step 6: Extract geographic places and get coordinates\n",
    "def extract_locations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Simple location patterns (can be enhanced)\n",
    "    locations = []\n",
    "    location_patterns = [\n",
    "        r'\\b(?:in|at|from)\\s+([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)',  # Places after prepositions\n",
    "        r'\\b([A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*)\\b'  # Capitalized names\n",
    "    ]\n",
    "    \n",
    "    for pattern in location_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        locations.extend(matches)\n",
    "    \n",
    "    # Filter common non-location capitalized words\n",
    "    common_words = {'The', 'A', 'An', 'I', 'Trump', 'Putin', 'America', 'USA', 'Russia', 'Ukraine', 'WW2', 'WW3'}\n",
    "    locations = [loc for loc in locations if loc not in common_words]\n",
    "    \n",
    "    return list(set(locations))\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        g = geocoder.osm(location)\n",
    "        if g.ok:\n",
    "            return (g.lat, g.lng, location)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "tweets_df['extracted_locations'] = tweets_df['translated_text'].apply(extract_locations)\n",
    "tweets_df['location_coordinates'] = tweets_df['extracted_locations'].apply(\n",
    "    lambda locs: [get_coordinates(loc) for loc in locs if loc]\n",
    ")\n",
    "tweets_df['location_coordinates'] = tweets_df['location_coordinates'].apply(\n",
    "    lambda coords: [c for c in coords if c is not None]\n",
    ")\n",
    "\n",
    "# Create folium map\n",
    "def create_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    \n",
    "    # Add markers for each tweet with location\n",
    "    for idx, row in df.iterrows():\n",
    "        for lat, lon, loc_name in row['location_coordinates']:\n",
    "            popup_text = f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                popup=popup_text,\n",
    "                icon=folium.Icon(color='blue')\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Save the map\n",
    "    m.save('tweet_locations.html')\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create time-series map\n",
    "def create_time_series_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    \n",
    "    # Prepare data for TimestampedGeoJson\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for lat, lon, loc_name in row['location_coordinates']:\n",
    "            time_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            feature = {\n",
    "                'type': 'Feature',\n",
    "                'geometry': {\n",
    "                    'type': 'Point',\n",
    "                    'coordinates': [lon, lat]\n",
    "                },\n",
    "                'properties': {\n",
    "                    'time': time_str,\n",
    "                    'popup': f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                }\n",
    "            }\n",
    "            features.append(feature)\n",
    "    \n",
    "    # Add TimestampedGeoJson to map\n",
    "    if features:\n",
    "        TimestampedGeoJson(\n",
    "            {'type': 'FeatureCollection', 'features': features},\n",
    "            period='PT1H',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=False,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            time_slider_drag_update=True\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save the map\n",
    "    m.save('tweet_time_series.html')\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Step 7: Extract mentioned users (@username)\n",
    "def extract_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return list(set(mentions))\n",
    "\n",
    "tweets_df['mentioned_users'] = tweets_df['text'].apply(extract_mentions)\n",
    "\n",
    "# Step 8: Assess political inclination\n",
    "def assess_political_inclination(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'neutral'\n",
    "    \n",
    "    # Simple keyword-based approach\n",
    "    left_keywords = ['democrat', 'liberal', 'progressive', 'left', 'biden', 'harris']\n",
    "    right_keywords = ['republican', 'conservative', 'maga', 'trump', 'right']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    left_count = sum(1 for kw in left_keywords if kw in text_lower)\n",
    "    right_count = sum(1 for kw in right_keywords if kw in text_lower)\n",
    "    \n",
    "    if left_count > right_count:\n",
    "        return 'left-leaning'\n",
    "    elif right_count > left_count:\n",
    "        return 'right-leaning'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "tweets_df['political_inclination'] = tweets_df['translated_text'].apply(assess_political_inclination)\n",
    "\n",
    "# Step 9: Create word cloud and co-occurrence heatmap\n",
    "def create_wordcloud(keywords):\n",
    "    # Extract words from keyword tuples and create frequency dict\n",
    "    word_freq = {}\n",
    "    for tweet_keywords in keywords:\n",
    "        for word, score in tweet_keywords:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += score\n",
    "            else:\n",
    "                word_freq[word] = score\n",
    "    \n",
    "    # Generate word cloud\n",
    "    if word_freq:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wordcloud.png')\n",
    "        plt.close()\n",
    "\n",
    "def create_cooccurrence_heatmap(df):\n",
    "    # Extract all unique keywords\n",
    "    all_keywords = set()\n",
    "    for kw_list in df['keywords']:\n",
    "        all_keywords.update([kw for kw, _ in kw_list])\n",
    "    \n",
    "    all_keywords = list(all_keywords)\n",
    "    \n",
    "    # Create co-occurrence matrix\n",
    "    cooccurrence = np.zeros((len(all_keywords), len(all_keywords)))\n",
    "    \n",
    "    for kw_list in df['keywords']:\n",
    "        words = [kw for kw, _ in kw_list]\n",
    "        for i, word1 in enumerate(all_keywords):\n",
    "            for j, word2 in enumerate(all_keywords):\n",
    "                if word1 in words and word2 in words and i != j:\n",
    "                    cooccurrence[i, j] += 1\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cooccurrence, xticklabels=all_keywords, yticklabels=all_keywords, cmap='Blues')\n",
    "    plt.title('Keyword Co-occurrence Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cooccurrence_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 10: Time series analysis\n",
    "def create_sentiment_time_series(df):\n",
    "    # Resample to hourly data\n",
    "    df_time = df.set_index('timestamp')\n",
    "    hourly_sentiment = df_time['sentence_sentiment'].resample('H').mean()\n",
    "    \n",
    "    # Plot time series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_sentiment.plot()\n",
    "    plt.title('Hourly Tweet Sentiment')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_time_series.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return hourly_sentiment\n",
    "\n",
    "# Step 11: Topic modeling\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    # Vectorize text\n",
    "    count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(texts)\n",
    "    \n",
    "    # LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    feature_names = count_vect.get_feature_names_out()\n",
    "    topics = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-10 - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", top_words))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Step 12: Create additional visualizations\n",
    "def create_top_visualizations(df):\n",
    "    # Top languages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['language_code'].value_counts().plot(kind='bar')\n",
    "    plt.title('Top Languages')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_languages.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Top mentioned users\n",
    "    all_mentions = []\n",
    "    for mentions in df['mentioned_users']:\n",
    "        all_mentions.extend(mentions)\n",
    "    \n",
    "    if all_mentions:\n",
    "        mention_counts = pd.Series(all_mentions).value_counts().head(10)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mention_counts.plot(kind='bar')\n",
    "        plt.title('Top Mentioned Users')\n",
    "        plt.xlabel('Username')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top_mentions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Political inclination distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['political_inclination'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Political Inclination Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('political_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Sentiment vs Political inclination scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {'left-leaning': 'blue', 'right-leaning': 'red', 'neutral': 'green'}\n",
    "    for inclination in colors:\n",
    "        subset = df[df['political_inclination'] == inclination]\n",
    "        plt.scatter(\n",
    "            subset.index, \n",
    "            subset['sentence_sentiment'],\n",
    "            c=colors[inclination],\n",
    "            label=inclination,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    plt.title('Sentiment vs Political Inclination')\n",
    "    plt.xlabel('Tweet Index (Time Ordered)')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_vs_politics.png')\n",
    "    plt.close()\n",
    "\n",
    "# Execute all analysis functions\n",
    "print(\"\\nCreating wordcloud...\")\n",
    "create_wordcloud(tweets_df['keywords'])\n",
    "\n",
    "print(\"Creating co-occurrence heatmap...\")\n",
    "create_cooccurrence_heatmap(tweets_df)\n",
    "\n",
    "print(\"Creating maps...\")\n",
    "create_map(tweets_df)\n",
    "create_time_series_map(tweets_df)\n",
    "\n",
    "print(\"Creating time series analysis...\")\n",
    "sentiment_ts = create_sentiment_time_series(tweets_df)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "topics = perform_topic_modeling(tweets_df['translated_text'].fillna(''), n_topics=5)\n",
    "for topic_name, top_words in topics:\n",
    "    print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "create_top_visualizations(tweets_df)\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\nGenerating AI summary report...\")\n",
    "\n",
    "# Calculate some statistics for the report\n",
    "total_tweets = len(tweets_df)\n",
    "unique_users = tweets_df['username'].nunique()\n",
    "avg_sentiment = tweets_df['sentence_sentiment'].mean()\n",
    "political_counts = tweets_df['political_inclination'].value_counts()\n",
    "\n",
    "# Generate a report summary\n",
    "report = f\"\"\"\n",
    "# Twitter Data Analysis Report\n",
    "\n",
    "## Overview\n",
    "- Total Tweets Analyzed: {total_tweets}\n",
    "- Unique Users: {unique_users}\n",
    "- Average Sentiment Score: {avg_sentiment:.2f}\n",
    "\n",
    "## Political Distribution\n",
    "{political_counts.to_string()}\n",
    "\n",
    "## Top Topics\n",
    "\"\"\"\n",
    "\n",
    "for topic_name, top_words in topics:\n",
    "    report += f\"- {topic_name}: {', '.join(top_words)}\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Key Findings\n",
    "- The dataset primarily contains tweets related to World War discussions, with many comparing WW2 and potential WW3 scenarios.\n",
    "- There's significant political polarization in discussions about Ukraine and Russia.\n",
    "- Multiple languages were detected, primarily English with some non-English content.\n",
    "- Sentiment analysis shows varying emotional responses to geopolitical topics.\n",
    "- Geographic analysis reveals global interest in these topics.\n",
    "\n",
    "## Generated Visualizations\n",
    "- Wordcloud of key terms\n",
    "- Co-occurrence heatmap showing related terms\n",
    "- Interactive map of tweet locations\n",
    "- Time series of tweet sentiments\n",
    "- Political inclination distribution\n",
    "- Sentiment vs politics scatter plot\n",
    "\"\"\"\n",
    "\n",
    "# Save the report\n",
    "with open('twitter_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete! All visualizations and the report have been saved.\")\n",
    "\n",
    "# Create final DataFrame with all analysis\n",
    "final_df = tweets_df[['username', 'text', 'timestamp', 'language_code', 'translated_text', \n",
    "                      'keywords', 'sentence_sentiment', 'extracted_locations', \n",
    "                      'mentioned_users', 'political_inclination']]\n",
    "\n",
    "# Save the enriched dataframe\n",
    "final_df.to_csv('analyzed_tweets.csv', index=False)\n",
    "\n",
    "print(\"\\nEnriched dataset saved to 'analyzed_tweets.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Dataset:\n",
      "Shape: (33, 9)\n",
      "\n",
      "Users Dataset:\n",
      "Shape: (27, 6)\n",
      "\n",
      "Creating wordcloud...\n",
      "Creating co-occurrence heatmap...\n",
      "Creating maps...\n",
      "Creating time series analysis...\n",
      "Performing topic modeling...\n",
      "Topic 1: ww2, churchill, white, military, winston, did, people, generation, allied, ussr\n",
      "Topic 2: ww3, right, wants, war, trump, think, going, european, russia, europe\n",
      "Topic 3: world, ww2, like, fight, war, long, ussr, allied, generation, think\n",
      "Topic 4: europe, ww3, need, ww2, don, america, weapons, ukraine, fighting, won\n",
      "Topic 5: ww2, war, did, fought, nation, hasn, want, order, start, allies\n",
      "Creating additional visualizations...\n",
      "\n",
      "Generating AI summary report...\n",
      "Analysis complete! All visualizations and the report have been saved.\n",
      "\n",
      "Enriched dataset saved to 'analyzed_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import and load spaCy and geopy\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Download necessary nltk resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize geopy Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"tweet_geocoder\")\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "\n",
    "# Display basic dataset info\n",
    "print(\"Tweets Dataset:\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(\"\\nUsers Dataset:\")\n",
    "print(f\"Shape: {users_df.shape}\")\n",
    "\n",
    "# Step 1: Sort data based on timestamps\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
    "tweets_df = tweets_df.sort_values(by='timestamp')\n",
    "\n",
    "# Step 2: Detect language using TextBlob and add language code column\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            return TextBlob(text).detect_language()\n",
    "        except Exception:\n",
    "            return 'en'  # default to English if detection fails\n",
    "    return 'en'\n",
    "\n",
    "tweets_df['language_code'] = tweets_df['text'].apply(detect_language)\n",
    "\n",
    "# Step 3: Translate non-English text to English using TextBlob\n",
    "def translate_to_english(row):\n",
    "    if row['language_code'] != 'en' and isinstance(row['text'], str):\n",
    "        try:\n",
    "            return str(TextBlob(row['text']).translate(to='en'))\n",
    "        except Exception:\n",
    "            return row['text']\n",
    "    return row['text']\n",
    "\n",
    "tweets_df['translated_text'] = tweets_df.apply(translate_to_english, axis=1)\n",
    "\n",
    "# Use translated_text for further analysis (fill missing values with an empty string)\n",
    "analysis_text = tweets_df['translated_text'].fillna('')\n",
    "\n",
    "# Step 4: Extract keywords using TF-IDF\n",
    "# Convert stopwords (originally a set) into a list to avoid InvalidParameterError.\n",
    "stop_words = list(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(max_features=5, stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(analysis_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(tfidf_row, feature_names):\n",
    "    indices = np.argsort(tfidf_row)[::-1]\n",
    "    top_keywords = [(feature_names[i], tfidf_row[i]) for i in indices if tfidf_row[i] > 0]\n",
    "    return top_keywords\n",
    "\n",
    "tweets_df['keywords'] = [get_top_keywords(tfidf_matrix[i].toarray()[0], feature_names) \n",
    "                           for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Step 5: Compute sentiment scores for each tweet and for each extracted keyword\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "tweets_df['keyword_sentiments'] = tweets_df['keywords'].apply(\n",
    "    lambda kw_list: [(kw, score, get_sentiment(kw)) for kw, score in kw_list] if isinstance(kw_list, list) else []\n",
    ")\n",
    "tweets_df['sentence_sentiment'] = tweets_df['translated_text'].apply(get_sentiment)\n",
    "\n",
    "# Step 6: Extract geographic locations using spaCy NER and get coordinates using geopy\n",
    "def extract_locations(text):\n",
    "    \"\"\"Use spaCy's NER to extract locations (GPE or LOC).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return list(set(locations))  # unique locations\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location)\n",
    "        if loc:\n",
    "            return (loc.latitude, loc.longitude, location)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "tweets_df['extracted_locations'] = tweets_df['translated_text'].apply(extract_locations)\n",
    "tweets_df['location_coordinates'] = tweets_df['extracted_locations'].apply(\n",
    "    lambda locs: [get_coordinates(loc) for loc in locs if loc]\n",
    ")\n",
    "tweets_df['location_coordinates'] = tweets_df['location_coordinates'].apply(\n",
    "    lambda coords: [c for c in coords if c is not None]\n",
    ")\n",
    "\n",
    "# Create a folium map with tweet locations\n",
    "def create_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    for idx, row in df.iterrows():\n",
    "        for coordinate in row['location_coordinates']:\n",
    "            if coordinate:\n",
    "                lat, lon, loc_name = coordinate\n",
    "                popup_text = f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                folium.Marker(\n",
    "                    location=[lat, lon],\n",
    "                    popup=popup_text,\n",
    "                    icon=folium.Icon(color='blue')\n",
    "                ).add_to(m)\n",
    "    m.save('tweet_locations.html')\n",
    "    return m\n",
    "\n",
    "# Create a time-series folium map with timestamped tweet locations\n",
    "def create_time_series_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for coordinate in row['location_coordinates']:\n",
    "            if coordinate:\n",
    "                lat, lon, loc_name = coordinate\n",
    "                time_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                feature = {\n",
    "                    'type': 'Feature',\n",
    "                    'geometry': {\n",
    "                        'type': 'Point',\n",
    "                        'coordinates': [lon, lat]\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'time': time_str,\n",
    "                        'popup': f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                    }\n",
    "                }\n",
    "                features.append(feature)\n",
    "    if features:\n",
    "        TimestampedGeoJson(\n",
    "            {'type': 'FeatureCollection', 'features': features},\n",
    "            period='PT1H',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=False,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            time_slider_drag_update=True\n",
    "        ).add_to(m)\n",
    "    m.save('tweet_time_series.html')\n",
    "    return m\n",
    "\n",
    "# Step 7: Extract mentioned users using regex\n",
    "def extract_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return list(set(mentions))\n",
    "\n",
    "tweets_df['mentioned_users'] = tweets_df['text'].apply(extract_mentions)\n",
    "\n",
    "# Step 8: Assess political inclination via a keyword-based approach\n",
    "def assess_political_inclination(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'neutral'\n",
    "    left_keywords = ['democrat', 'liberal', 'progressive', 'left', 'biden', 'harris']\n",
    "    right_keywords = ['republican', 'conservative', 'maga', 'trump', 'right']\n",
    "    text_lower = text.lower()\n",
    "    left_count = sum(1 for kw in left_keywords if kw in text_lower)\n",
    "    right_count = sum(1 for kw in right_keywords if kw in text_lower)\n",
    "    if left_count > right_count:\n",
    "        return 'left-leaning'\n",
    "    elif right_count > left_count:\n",
    "        return 'right-leaning'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "tweets_df['political_inclination'] = tweets_df['translated_text'].apply(assess_political_inclination)\n",
    "\n",
    "# Step 9: Create a word cloud and a keyword co-occurrence heatmap\n",
    "def create_wordcloud(keywords):\n",
    "    word_freq = {}\n",
    "    for tweet_keywords in keywords:\n",
    "        for word, score in tweet_keywords:\n",
    "            word_freq[word] = word_freq.get(word, 0) + score\n",
    "    if word_freq:\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wordcloud.png')\n",
    "        plt.close()\n",
    "\n",
    "def create_cooccurrence_heatmap(df):\n",
    "    all_keywords = set()\n",
    "    for kw_list in df['keywords']:\n",
    "        all_keywords.update([kw for kw, _ in kw_list])\n",
    "    all_keywords = list(all_keywords)\n",
    "    \n",
    "    cooccurrence = np.zeros((len(all_keywords), len(all_keywords)))\n",
    "    for kw_list in df['keywords']:\n",
    "        words = [kw for kw, _ in kw_list]\n",
    "        for i, word1 in enumerate(all_keywords):\n",
    "            for j, word2 in enumerate(all_keywords):\n",
    "                if word1 in words and word2 in words and i != j:\n",
    "                    cooccurrence[i, j] += 1\n",
    "                    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cooccurrence, xticklabels=all_keywords, yticklabels=all_keywords, cmap='Blues')\n",
    "    plt.title('Keyword Co-occurrence Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cooccurrence_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 10: Time series analysis of tweet sentiment\n",
    "def create_sentiment_time_series(df):\n",
    "    df_time = df.set_index('timestamp')\n",
    "    hourly_sentiment = df_time['sentence_sentiment'].resample('H').mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_sentiment.plot()\n",
    "    plt.title('Hourly Tweet Sentiment')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_time_series.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return hourly_sentiment\n",
    "\n",
    "# Step 11: Perform topic modeling using LDA\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    \n",
    "    feature_names = count_vect.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", top_words))\n",
    "    return topics\n",
    "\n",
    "# Step 12: Additional visualizations (top languages, top mentions, political distribution, sentiment vs. politics)\n",
    "def create_top_visualizations(df):\n",
    "    # Top languages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['language_code'].value_counts().plot(kind='bar')\n",
    "    plt.title('Top Languages')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_languages.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Top mentioned users\n",
    "    all_mentions = []\n",
    "    for mentions in df['mentioned_users']:\n",
    "        all_mentions.extend(mentions)\n",
    "    \n",
    "    if all_mentions:\n",
    "        mention_counts = pd.Series(all_mentions).value_counts().head(10)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mention_counts.plot(kind='bar')\n",
    "        plt.title('Top Mentioned Users')\n",
    "        plt.xlabel('Username')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top_mentions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Political inclination distribution (pie chart)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['political_inclination'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Political Inclination Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('political_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter plot: Sentiment vs. Political inclination\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {'left-leaning': 'blue', 'right-leaning': 'red', 'neutral': 'green'}\n",
    "    for inclination, color in colors.items():\n",
    "        subset = df[df['political_inclination'] == inclination]\n",
    "        plt.scatter(\n",
    "            subset.index, \n",
    "            subset['sentence_sentiment'],\n",
    "            c=color,\n",
    "            label=inclination,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    plt.title('Sentiment vs Political Inclination')\n",
    "    plt.xlabel('Tweet Index (Time Ordered)')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_vs_politics.png')\n",
    "    plt.close()\n",
    "\n",
    "# Run all analysis functions\n",
    "print(\"\\nCreating wordcloud...\")\n",
    "create_wordcloud(tweets_df['keywords'])\n",
    "\n",
    "print(\"Creating co-occurrence heatmap...\")\n",
    "create_cooccurrence_heatmap(tweets_df)\n",
    "\n",
    "print(\"Creating maps...\")\n",
    "create_map(tweets_df)\n",
    "create_time_series_map(tweets_df)\n",
    "\n",
    "print(\"Creating time series analysis...\")\n",
    "sentiment_ts = create_sentiment_time_series(tweets_df)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "topics = perform_topic_modeling(tweets_df['translated_text'].fillna(''), n_topics=5)\n",
    "for topic_name, top_words in topics:\n",
    "    print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "create_top_visualizations(tweets_df)\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\nGenerating AI summary report...\")\n",
    "\n",
    "total_tweets = len(tweets_df)\n",
    "unique_users = tweets_df['username'].nunique()\n",
    "avg_sentiment = tweets_df['sentence_sentiment'].mean()\n",
    "political_counts = tweets_df['political_inclination'].value_counts()\n",
    "\n",
    "report = f\"\"\"\n",
    "# Twitter Data Analysis Report\n",
    "\n",
    "## Overview\n",
    "- Total Tweets Analyzed: {total_tweets}\n",
    "- Unique Users: {unique_users}\n",
    "- Average Sentiment Score: {avg_sentiment:.2f}\n",
    "\n",
    "## Political Distribution\n",
    "{political_counts.to_string()}\n",
    "\n",
    "## Top Topics\n",
    "\"\"\"\n",
    "\n",
    "for topic_name, top_words in topics:\n",
    "    report += f\"- {topic_name}: {', '.join(top_words)}\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Key Findings\n",
    "- The dataset contains a mix of languages, with non-English tweets translated to English for uniform analysis.\n",
    "- SpaCyâ€™s NER was used to extract geographic locations, which were then geocoded using geopy.\n",
    "- Sentiment analysis shows varied emotional responses across tweets.\n",
    "- Topic modeling using LDA identified distinct themes within the data.\n",
    "\n",
    "## Generated Visualizations\n",
    "- Wordcloud of key terms\n",
    "- Co-occurrence heatmap of keywords\n",
    "- Interactive maps of tweet locations (static and time series)\n",
    "- Time series of tweet sentiments\n",
    "- Political inclination distribution and sentiment vs. politics scatter plot\n",
    "\"\"\"\n",
    "\n",
    "with open('twitter_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete! All visualizations and the report have been saved.\")\n",
    "\n",
    "# Create a final DataFrame with enriched analysis columns\n",
    "final_df = tweets_df[['username', 'text', 'timestamp', 'language_code', 'translated_text', \n",
    "                        'keywords', 'sentence_sentiment', 'extracted_locations', \n",
    "                        'mentioned_users', 'political_inclination']]\n",
    "\n",
    "final_df.to_csv('analyzed_tweets.csv', index=False)\n",
    "print(\"\\nEnriched dataset saved to 'analyzed_tweets.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Dataset:\n",
      "Shape: (33, 9)\n",
      "\n",
      "Users Dataset:\n",
      "Shape: (27, 6)\n",
      "\n",
      "Creating wordcloud...\n",
      "Creating co-occurrence heatmap...\n",
      "Creating user location map...\n",
      "Creating tweet time-series map...\n",
      "Plotting sentiment trend line...\n",
      "Plotting daily tweet count trend...\n",
      "Performing topic modeling...\n",
      "Topic 1: ww2, churchill, did, military, white, winston, don, people, generation, order\n",
      "Topic 2: europe, ww2, right, ww3, need, nazis, ussr, america, don, going\n",
      "Topic 3: ww3, wants, fighting, won, weapons, going, ukraine, people, nation, hasn\n",
      "Topic 4: ww2, world, like, think, need, europe, war, america, saved, don\n",
      "Topic 5: ww2, war, start, world, won, allied, grandfather, allies, order, ussr\n",
      "Creating additional visualizations...\n",
      "\n",
      "Generating AI summary report...\n",
      "Analysis complete! All visualizations and the report have been saved.\n",
      "\n",
      "Enriched dataset saved to 'analyzed_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import language detection and translation libraries\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "# Import spaCy and geopy for location extraction and geocoding\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize geopy Nominatim geocoder and googletrans translator\n",
    "geolocator = Nominatim(user_agent=\"tweet_geocoder\")\n",
    "translator = Translator()\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "\n",
    "# Display basic dataset info\n",
    "print(\"Tweets Dataset:\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(\"\\nUsers Dataset:\")\n",
    "print(f\"Shape: {users_df.shape}\")\n",
    "\n",
    "# Step 1: Sort tweets by timestamp\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
    "tweets_df = tweets_df.sort_values(by='timestamp')\n",
    "\n",
    "# Step 2: Detect language using langdetect\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except Exception:\n",
    "            return 'en'\n",
    "    return 'en'\n",
    "\n",
    "tweets_df['language_code'] = tweets_df['text'].apply(detect_language)\n",
    "\n",
    "# Step 3: Translate non-English text to English using googletrans\n",
    "def translate_to_english(row):\n",
    "    if row['language_code'] != 'en' and isinstance(row['text'], str):\n",
    "        try:\n",
    "            translation = translator.translate(row['text'], dest='en')\n",
    "            return translation.text\n",
    "        except Exception:\n",
    "            return row['text']\n",
    "    return row['text']\n",
    "\n",
    "tweets_df['translated_text'] = tweets_df.apply(translate_to_english, axis=1)\n",
    "analysis_text = tweets_df['translated_text'].fillna('')\n",
    "\n",
    "# Step 4: Extract keywords using TF-IDF\n",
    "# Convert NLTK stopwords (set) to a list\n",
    "stop_words = list(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(max_features=5, stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(analysis_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(tfidf_row, feature_names):\n",
    "    indices = np.argsort(tfidf_row)[::-1]\n",
    "    top_keywords = [(feature_names[i], tfidf_row[i]) for i in indices if tfidf_row[i] > 0]\n",
    "    return top_keywords\n",
    "\n",
    "tweets_df['keywords'] = [get_top_keywords(tfidf_matrix[i].toarray()[0], feature_names) \n",
    "                           for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Step 5: Compute sentiment scores (for each tweet and for keywords)\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "tweets_df['keyword_sentiments'] = tweets_df['keywords'].apply(\n",
    "    lambda kw_list: [(kw, score, get_sentiment(kw)) for kw, score in kw_list] if isinstance(kw_list, list) else []\n",
    ")\n",
    "tweets_df['sentence_sentiment'] = tweets_df['translated_text'].apply(get_sentiment)\n",
    "\n",
    "# Step 6: Extract geographic locations using spaCy NER and geocode them\n",
    "def extract_locations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    # Extract entities labeled as GPE (Countries, cities) or LOC\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return list(set(locations))  # Return unique locations\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location)\n",
    "        if loc:\n",
    "            return (loc.latitude, loc.longitude, location)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "tweets_df['extracted_locations'] = tweets_df['translated_text'].apply(extract_locations)\n",
    "tweets_df['location_coordinates'] = tweets_df['extracted_locations'].apply(\n",
    "    lambda locs: [get_coordinates(loc) for loc in locs if loc]\n",
    ")\n",
    "tweets_df['location_coordinates'] = tweets_df['location_coordinates'].apply(\n",
    "    lambda coords: [c for c in coords if c is not None]\n",
    ")\n",
    "\n",
    "# Step 7: Extract mentioned users using regex\n",
    "def extract_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return list(set(mentions))\n",
    "\n",
    "tweets_df['mentioned_users'] = tweets_df['text'].apply(extract_mentions)\n",
    "\n",
    "# Step 8: Assess political inclination via a keyword-based approach\n",
    "def assess_political_inclination(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'neutral'\n",
    "    left_keywords = ['democrat', 'liberal', 'progressive', 'left', 'biden', 'harris']\n",
    "    right_keywords = ['republican', 'conservative', 'maga', 'trump', 'right']\n",
    "    text_lower = text.lower()\n",
    "    left_count = sum(1 for kw in left_keywords if kw in text_lower)\n",
    "    right_count = sum(1 for kw in right_keywords if kw in text_lower)\n",
    "    if left_count > right_count:\n",
    "        return 'left-leaning'\n",
    "    elif right_count > left_count:\n",
    "        return 'right-leaning'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "tweets_df['political_inclination'] = tweets_df['translated_text'].apply(assess_political_inclination)\n",
    "\n",
    "# Step 9: Create wordcloud and co-occurrence heatmap of keywords\n",
    "def create_wordcloud(keywords):\n",
    "    word_freq = {}\n",
    "    for tweet_keywords in keywords:\n",
    "        for word, score in tweet_keywords:\n",
    "            word_freq[word] = word_freq.get(word, 0) + score\n",
    "    if word_freq:\n",
    "        wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wordcloud.png')\n",
    "        plt.close()\n",
    "\n",
    "def create_cooccurrence_heatmap(df):\n",
    "    all_keywords = set()\n",
    "    for kw_list in df['keywords']:\n",
    "        all_keywords.update([kw for kw, _ in kw_list])\n",
    "    all_keywords = list(all_keywords)\n",
    "    \n",
    "    cooccurrence = np.zeros((len(all_keywords), len(all_keywords)))\n",
    "    for kw_list in df['keywords']:\n",
    "        words = [kw for kw, _ in kw_list]\n",
    "        for i, word1 in enumerate(all_keywords):\n",
    "            for j, word2 in enumerate(all_keywords):\n",
    "                if word1 in words and word2 in words and i != j:\n",
    "                    cooccurrence[i, j] += 1\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cooccurrence, xticklabels=all_keywords, yticklabels=all_keywords, cmap='Blues')\n",
    "    plt.title('Keyword Co-occurrence Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cooccurrence_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 10: Create a linear regression trend plot for tweet sentiment over a numeric sequence\n",
    "def plot_trend_line(df, column, title, filename):\n",
    "    # Create a numeric sequence based on the tweet order (index)\n",
    "    X = np.arange(len(df)).reshape(-1, 1)\n",
    "    Y = df[column].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, Y, label='Data', alpha=0.5)\n",
    "    plt.plot(X, trend, color='red', label='Trend line')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Tweet Index (Numeric Sequence)\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Step 11: Plot time series for number of tweets (daily count) with linear regression trend\n",
    "def plot_tweet_count_trend(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = df_copy['timestamp'].dt.date\n",
    "    tweet_count = df_copy.groupby('date').size().reset_index(name='count')\n",
    "    X = np.arange(len(tweet_count)).reshape(-1, 1)\n",
    "    Y = tweet_count['count'].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(tweet_count['date'], Y, label='Daily Tweet Count', alpha=0.5)\n",
    "    plt.plot(tweet_count['date'], trend, color='red', label='Trend Line')\n",
    "    plt.title('Daily Tweet Count with Trend Line')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tweet Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('daily_tweet_count_trend.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 12: Create maps\n",
    "# Map One: Map of user locations (using users_df and their \"location\" field)\n",
    "def create_user_location_map(users_df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    # Assumes users_df contains columns 'username' and 'location'\n",
    "    for idx, row in users_df.iterrows():\n",
    "        if pd.notnull(row.get('location')):\n",
    "            try:\n",
    "                loc = geolocator.geocode(row['location'])\n",
    "                if loc:\n",
    "                    folium.Marker(\n",
    "                        location=[loc.latitude, loc.longitude],\n",
    "                        popup=f\"User: {row['username']}<br>Location: {row['location']}\",\n",
    "                        icon=folium.Icon(color='green')\n",
    "                    ).add_to(m)\n",
    "            except Exception:\n",
    "                continue\n",
    "    m.save('user_locations_map.html')\n",
    "    return m\n",
    "\n",
    "# Map Two: Time-series map of extracted tweet locations\n",
    "def create_time_series_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for coordinate in row['location_coordinates']:\n",
    "            if coordinate:\n",
    "                lat, lon, loc_name = coordinate\n",
    "                time_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                feature = {\n",
    "                    'type': 'Feature',\n",
    "                    'geometry': {\n",
    "                        'type': 'Point',\n",
    "                        'coordinates': [lon, lat]\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'time': time_str,\n",
    "                        'popup': f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                    }\n",
    "                }\n",
    "                features.append(feature)\n",
    "    if features:\n",
    "        TimestampedGeoJson(\n",
    "            {'type': 'FeatureCollection', 'features': features},\n",
    "            period='PT1H',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=False,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            time_slider_drag_update=True\n",
    "        ).add_to(m)\n",
    "    m.save('tweet_time_series_map.html')\n",
    "    return m\n",
    "\n",
    "# Step 13: Topic modeling using LDA (unchanged from before)\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    feature_names = count_vect.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", top_words))\n",
    "    return topics\n",
    "\n",
    "# Step 14: Additional visualizations for top entities\n",
    "def create_top_visualizations(df):\n",
    "    # Top languages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['language_code'].value_counts().plot(kind='bar')\n",
    "    plt.title('Top Languages')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_languages.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Top mentioned users\n",
    "    all_mentions = []\n",
    "    for mentions in df['mentioned_users']:\n",
    "        all_mentions.extend(mentions)\n",
    "    if all_mentions:\n",
    "        mention_counts = pd.Series(all_mentions).value_counts().head(10)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mention_counts.plot(kind='bar')\n",
    "        plt.title('Top Mentioned Users')\n",
    "        plt.xlabel('Username')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top_mentions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Political inclination distribution (pie chart)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['political_inclination'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Political Inclination Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('political_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter plot: Sentiment vs. Political inclination\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {'left-leaning': 'blue', 'right-leaning': 'red', 'neutral': 'green'}\n",
    "    for inclination, color in colors.items():\n",
    "        subset = df[df['political_inclination'] == inclination]\n",
    "        plt.scatter(\n",
    "            subset.index, \n",
    "            subset['sentence_sentiment'],\n",
    "            c=color,\n",
    "            label=inclination,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    plt.title('Sentiment vs Political Inclination')\n",
    "    plt.xlabel('Tweet Index (Time Ordered)')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_vs_politics.png')\n",
    "    plt.close()\n",
    "\n",
    "# ------------------ Execute All Analysis Steps ------------------\n",
    "\n",
    "print(\"\\nCreating wordcloud...\")\n",
    "create_wordcloud(tweets_df['keywords'])\n",
    "\n",
    "print(\"Creating co-occurrence heatmap...\")\n",
    "create_cooccurrence_heatmap(tweets_df)\n",
    "\n",
    "print(\"Creating user location map...\")\n",
    "create_user_location_map(users_df)\n",
    "\n",
    "print(\"Creating tweet time-series map...\")\n",
    "create_time_series_map(tweets_df)\n",
    "\n",
    "print(\"Plotting sentiment trend line...\")\n",
    "plot_trend_line(tweets_df, 'sentence_sentiment', 'Tweet Sentiment Trend', 'sentiment_trend.png')\n",
    "\n",
    "print(\"Plotting daily tweet count trend...\")\n",
    "plot_tweet_count_trend(tweets_df)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "topics = perform_topic_modeling(tweets_df['translated_text'].fillna(''), n_topics=5)\n",
    "for topic_name, top_words in topics:\n",
    "    print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "create_top_visualizations(tweets_df)\n",
    "\n",
    "# Generate comprehensive AI summary report\n",
    "print(\"\\nGenerating AI summary report...\")\n",
    "\n",
    "total_tweets = len(tweets_df)\n",
    "unique_users = tweets_df['username'].nunique()\n",
    "avg_sentiment = tweets_df['sentence_sentiment'].mean()\n",
    "political_counts = tweets_df['political_inclination'].value_counts()\n",
    "\n",
    "report = f\"\"\"\n",
    "# Twitter Data Analysis Report\n",
    "\n",
    "## Overview\n",
    "- Total Tweets Analyzed: {total_tweets}\n",
    "- Unique Users: {unique_users}\n",
    "- Average Sentiment Score: {avg_sentiment:.2f}\n",
    "\n",
    "## Political Distribution\n",
    "{political_counts.to_string()}\n",
    "\n",
    "## Top Topics\n",
    "\"\"\"\n",
    "\n",
    "for topic_name, top_words in topics:\n",
    "    report += f\"- {topic_name}: {', '.join(top_words)}\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Key Findings\n",
    "- Non-English tweets are detected using langdetect and translated to English via googletrans.\n",
    "- Two maps are generated: one showing user locations and one as a time-series map of tweet-extracted locations.\n",
    "- Linear regression on the tweet index (numeric sequence) provides a trend line for sentiment, and daily tweet counts are analyzed.\n",
    "- Topic modeling (LDA) identifies distinct themes within the tweets.\n",
    "\n",
    "## Generated Visualizations\n",
    "- Wordcloud of keywords\n",
    "- Keyword co-occurrence heatmap\n",
    "- User locations map\n",
    "- Time-series map of tweet locations\n",
    "- Sentiment trend line (with linear regression)\n",
    "- Daily tweet count trend\n",
    "- Political inclination distribution and sentiment vs. politics scatter plot\n",
    "\"\"\"\n",
    "\n",
    "with open('twitter_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete! All visualizations and the report have been saved.\")\n",
    "\n",
    "# Create a final enriched DataFrame and save it\n",
    "final_df = tweets_df[['username', 'text', 'timestamp', 'language_code', 'translated_text', \n",
    "                        'keywords', 'sentence_sentiment', 'extracted_locations', \n",
    "                        'mentioned_users', 'political_inclination']]\n",
    "final_df.to_csv('analyzed_tweets.csv', index=False)\n",
    "print(\"\\nEnriched dataset saved to 'analyzed_tweets.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Dataset:\n",
      "Shape: (33, 9)\n",
      "\n",
      "Users Dataset:\n",
      "Shape: (27, 6)\n",
      "\n",
      "Creating wordcloud...\n",
      "Creating co-occurrence heatmap...\n",
      "Creating user location map...\n",
      "Creating tweet time-series map...\n",
      "Plotting sentiment trend line...\n",
      "Plotting daily tweet count trend...\n",
      "Performing topic modeling...\n",
      "Topic 1: ww2, churchill, did, military, white, winston, don, people, generation, order\n",
      "Topic 2: europe, ww2, right, ww3, need, nazis, ussr, america, don, going\n",
      "Topic 3: ww3, wants, fighting, won, weapons, going, ukraine, people, nation, hasn\n",
      "Topic 4: ww2, world, like, think, need, europe, war, america, saved, don\n",
      "Topic 5: ww2, war, start, world, won, allied, grandfather, allies, order, ussr\n",
      "Creating additional visualizations...\n",
      "\n",
      "Generating AI summary report...\n",
      "Analysis complete! All visualizations and the report have been saved.\n",
      "\n",
      "Enriched dataset saved to 'analyzed_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import language detection and translation libraries\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "# Import spaCy and geopy for location extraction and geocoding\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize geopy Nominatim geocoder and googletrans translator\n",
    "geolocator = Nominatim(user_agent=\"tweet_geocoder\")\n",
    "translator = Translator()\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "\n",
    "# Display basic dataset info\n",
    "print(\"Tweets Dataset:\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(\"\\nUsers Dataset:\")\n",
    "print(f\"Shape: {users_df.shape}\")\n",
    "\n",
    "# Step 1: Sort tweets by timestamp\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
    "tweets_df = tweets_df.sort_values(by='timestamp')\n",
    "\n",
    "# Step 2: Detect language using langdetect\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except Exception:\n",
    "            return 'en'\n",
    "    return 'en'\n",
    "\n",
    "tweets_df['language_code'] = tweets_df['text'].apply(detect_language)\n",
    "\n",
    "# Step 3: Translate non-English text to English using googletrans\n",
    "def translate_to_english(row):\n",
    "    if row['language_code'] != 'en' and isinstance(row['text'], str):\n",
    "        try:\n",
    "            translation = translator.translate(row['text'], dest='en')\n",
    "            return translation.text\n",
    "        except Exception:\n",
    "            return row['text']\n",
    "    return row['text']\n",
    "\n",
    "tweets_df['translated_text'] = tweets_df.apply(translate_to_english, axis=1)\n",
    "analysis_text = tweets_df['translated_text'].fillna('')\n",
    "\n",
    "# Step 4: Extract keywords using TF-IDF\n",
    "# Convert NLTK stopwords (set) to a list\n",
    "stop_words = list(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(max_features=5, stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(analysis_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(tfidf_row, feature_names):\n",
    "    indices = np.argsort(tfidf_row)[::-1]\n",
    "    top_keywords = [(feature_names[i], tfidf_row[i]) for i in indices if tfidf_row[i] > 0]\n",
    "    return top_keywords\n",
    "\n",
    "tweets_df['keywords'] = [get_top_keywords(tfidf_matrix[i].toarray()[0], feature_names) \n",
    "                           for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Step 5: Compute sentiment scores (for each tweet and for keywords)\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "tweets_df['keyword_sentiments'] = tweets_df['keywords'].apply(\n",
    "    lambda kw_list: [(kw, score, get_sentiment(kw)) for kw, score in kw_list] if isinstance(kw_list, list) else []\n",
    ")\n",
    "tweets_df['sentence_sentiment'] = tweets_df['translated_text'].apply(get_sentiment)\n",
    "\n",
    "# Step 6: Extract geographic locations using spaCy NER and geocode them\n",
    "def extract_locations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    # Extract entities labeled as GPE (Countries, cities) or LOC\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return list(set(locations))  # Return unique locations\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location)\n",
    "        if loc:\n",
    "            return (loc.latitude, loc.longitude, location)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "tweets_df['extracted_locations'] = tweets_df['translated_text'].apply(extract_locations)\n",
    "tweets_df['location_coordinates'] = tweets_df['extracted_locations'].apply(\n",
    "    lambda locs: [get_coordinates(loc) for loc in locs if loc]\n",
    ")\n",
    "tweets_df['location_coordinates'] = tweets_df['location_coordinates'].apply(\n",
    "    lambda coords: [c for c in coords if c is not None]\n",
    ")\n",
    "\n",
    "# Step 7: Extract mentioned users using regex\n",
    "def extract_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return list(set(mentions))\n",
    "\n",
    "tweets_df['mentioned_users'] = tweets_df['text'].apply(extract_mentions)\n",
    "\n",
    "# Step 8: Assess political inclination via a keyword-based approach\n",
    "def assess_political_inclination(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'neutral'\n",
    "    left_keywords = ['democrat', 'liberal', 'progressive', 'left', 'biden', 'harris']\n",
    "    right_keywords = ['republican', 'conservative', 'maga', 'trump', 'right']\n",
    "    text_lower = text.lower()\n",
    "    left_count = sum(1 for kw in left_keywords if kw in text_lower)\n",
    "    right_count = sum(1 for kw in right_keywords if kw in text_lower)\n",
    "    if left_count > right_count:\n",
    "        return 'left-leaning'\n",
    "    elif right_count > left_count:\n",
    "        return 'right-leaning'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "tweets_df['political_inclination'] = tweets_df['translated_text'].apply(assess_political_inclination)\n",
    "\n",
    "# Step 9: Create wordcloud and co-occurrence heatmap of keywords\n",
    "def create_wordcloud(keywords):\n",
    "    word_freq = {}\n",
    "    for tweet_keywords in keywords:\n",
    "        for word, score in tweet_keywords:\n",
    "            word_freq[word] = word_freq.get(word, 0) + score\n",
    "    if word_freq:\n",
    "        wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wordcloud.png')\n",
    "        plt.close()\n",
    "\n",
    "def create_cooccurrence_heatmap(df):\n",
    "    all_keywords = set()\n",
    "    for kw_list in df['keywords']:\n",
    "        all_keywords.update([kw for kw, _ in kw_list])\n",
    "    all_keywords = list(all_keywords)\n",
    "    \n",
    "    cooccurrence = np.zeros((len(all_keywords), len(all_keywords)))\n",
    "    for kw_list in df['keywords']:\n",
    "        words = [kw for kw, _ in kw_list]\n",
    "        for i, word1 in enumerate(all_keywords):\n",
    "            for j, word2 in enumerate(all_keywords):\n",
    "                if word1 in words and word2 in words and i != j:\n",
    "                    cooccurrence[i, j] += 1\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cooccurrence, xticklabels=all_keywords, yticklabels=all_keywords, cmap='Blues')\n",
    "    plt.title('Keyword Co-occurrence Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cooccurrence_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 10: Create a linear regression trend plot for tweet sentiment over a numeric sequence\n",
    "def plot_trend_line(df, column, title, filename):\n",
    "    # Create a numeric sequence based on the tweet order (index)\n",
    "    X = np.arange(len(df)).reshape(-1, 1)\n",
    "    Y = df[column].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, Y, label='Data', alpha=0.5)\n",
    "    plt.plot(X, trend, color='red', label='Trend line')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Tweet Index (Numeric Sequence)\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Step 11: Plot time series for number of tweets (daily count) with linear regression trend\n",
    "def plot_tweet_count_trend(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = df_copy['timestamp'].dt.date\n",
    "    tweet_count = df_copy.groupby('date').size().reset_index(name='count')\n",
    "    X = np.arange(len(tweet_count)).reshape(-1, 1)\n",
    "    Y = tweet_count['count'].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(tweet_count['date'], Y, label='Daily Tweet Count', alpha=0.5)\n",
    "    plt.plot(tweet_count['date'], trend, color='red', label='Trend Line')\n",
    "    plt.title('Daily Tweet Count with Trend Line')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tweet Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('daily_tweet_count_trend.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 12: Create maps\n",
    "# Map One: Map of user locations (using users_df and their \"location\" field)\n",
    "def create_user_location_map(users_df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    # Assumes users_df contains columns 'username' and 'location'\n",
    "    for idx, row in users_df.iterrows():\n",
    "        if pd.notnull(row.get('location')):\n",
    "            try:\n",
    "                loc = geolocator.geocode(row['location'])\n",
    "                if loc:\n",
    "                    folium.Marker(\n",
    "                        location=[loc.latitude, loc.longitude],\n",
    "                        popup=f\"User: {row['username']}<br>Location: {row['location']}\",\n",
    "                        icon=folium.Icon(color='green')\n",
    "                    ).add_to(m)\n",
    "            except Exception:\n",
    "                continue\n",
    "    m.save('user_locations_map.html')\n",
    "    return m\n",
    "\n",
    "# Map Two: Time-series map of extracted tweet locations\n",
    "def create_time_series_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for coordinate in row['location_coordinates']:\n",
    "            if coordinate:\n",
    "                lat, lon, loc_name = coordinate\n",
    "                time_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                feature = {\n",
    "                    'type': 'Feature',\n",
    "                    'geometry': {\n",
    "                        'type': 'Point',\n",
    "                        'coordinates': [lon, lat]\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'time': time_str,\n",
    "                        'popup': f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                    }\n",
    "                }\n",
    "                features.append(feature)\n",
    "    if features:\n",
    "        TimestampedGeoJson(\n",
    "            {'type': 'FeatureCollection', 'features': features},\n",
    "            period='PT1H',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=False,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            time_slider_drag_update=True\n",
    "        ).add_to(m)\n",
    "    m.save('tweet_time_series_map.html')\n",
    "    return m\n",
    "\n",
    "# Step 13: Topic modeling using LDA (unchanged from before)\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    feature_names = count_vect.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", top_words))\n",
    "    return topics\n",
    "\n",
    "# Step 14: Additional visualizations for top entities\n",
    "def create_top_visualizations(df):\n",
    "    # Top languages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['language_code'].value_counts().plot(kind='bar')\n",
    "    plt.title('Top Languages')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_languages.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Top mentioned users\n",
    "    all_mentions = []\n",
    "    for mentions in df['mentioned_users']:\n",
    "        all_mentions.extend(mentions)\n",
    "    if all_mentions:\n",
    "        mention_counts = pd.Series(all_mentions).value_counts().head(10)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mention_counts.plot(kind='bar')\n",
    "        plt.title('Top Mentioned Users')\n",
    "        plt.xlabel('Username')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top_mentions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Political inclination distribution (pie chart)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['political_inclination'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Political Inclination Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('political_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter plot: Sentiment vs. Political inclination\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {'left-leaning': 'blue', 'right-leaning': 'red', 'neutral': 'green'}\n",
    "    for inclination, color in colors.items():\n",
    "        subset = df[df['political_inclination'] == inclination]\n",
    "        plt.scatter(\n",
    "            subset.index, \n",
    "            subset['sentence_sentiment'],\n",
    "            c=color,\n",
    "            label=inclination,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    plt.title('Sentiment vs Political Inclination')\n",
    "    plt.xlabel('Tweet Index (Time Ordered)')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_vs_politics.png')\n",
    "    plt.close()\n",
    "\n",
    "# ------------------ Execute All Analysis Steps ------------------\n",
    "\n",
    "print(\"\\nCreating wordcloud...\")\n",
    "create_wordcloud(tweets_df['keywords'])\n",
    "\n",
    "print(\"Creating co-occurrence heatmap...\")\n",
    "create_cooccurrence_heatmap(tweets_df)\n",
    "\n",
    "print(\"Creating user location map...\")\n",
    "create_user_location_map(users_df)\n",
    "\n",
    "print(\"Creating tweet time-series map...\")\n",
    "create_time_series_map(tweets_df)\n",
    "\n",
    "print(\"Plotting sentiment trend line...\")\n",
    "plot_trend_line(tweets_df, 'sentence_sentiment', 'Tweet Sentiment Trend', 'sentiment_trend.png')\n",
    "\n",
    "print(\"Plotting daily tweet count trend...\")\n",
    "plot_tweet_count_trend(tweets_df)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "topics = perform_topic_modeling(tweets_df['translated_text'].fillna(''), n_topics=5)\n",
    "for topic_name, top_words in topics:\n",
    "    print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "create_top_visualizations(tweets_df)\n",
    "\n",
    "# Generate comprehensive AI summary report\n",
    "print(\"\\nGenerating AI summary report...\")\n",
    "\n",
    "total_tweets = len(tweets_df)\n",
    "unique_users = tweets_df['username'].nunique()\n",
    "avg_sentiment = tweets_df['sentence_sentiment'].mean()\n",
    "political_counts = tweets_df['political_inclination'].value_counts()\n",
    "\n",
    "report = f\"\"\"\n",
    "# Twitter Data Analysis Report\n",
    "\n",
    "## Overview\n",
    "- Total Tweets Analyzed: {total_tweets}\n",
    "- Unique Users: {unique_users}\n",
    "- Average Sentiment Score: {avg_sentiment:.2f}\n",
    "\n",
    "## Political Distribution\n",
    "{political_counts.to_string()}\n",
    "\n",
    "## Top Topics\n",
    "\"\"\"\n",
    "\n",
    "for topic_name, top_words in topics:\n",
    "    report += f\"- {topic_name}: {', '.join(top_words)}\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Key Findings\n",
    "- Non-English tweets are detected using langdetect and translated to English via googletrans.\n",
    "- Two maps are generated: one showing user locations and one as a time-series map of tweet-extracted locations.\n",
    "- Linear regression on the tweet index (numeric sequence) provides a trend line for sentiment, and daily tweet counts are analyzed.\n",
    "- Topic modeling (LDA) identifies distinct themes within the tweets.\n",
    "\n",
    "## Generated Visualizations\n",
    "- Wordcloud of keywords\n",
    "- Keyword co-occurrence heatmap\n",
    "- User locations map\n",
    "- Time-series map of tweet locations\n",
    "- Sentiment trend line (with linear regression)\n",
    "- Daily tweet count trend\n",
    "- Political inclination distribution and sentiment vs. politics scatter plot\n",
    "\"\"\"\n",
    "\n",
    "with open('twitter_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete! All visualizations and the report have been saved.\")\n",
    "\n",
    "# Create a final enriched DataFrame and save it\n",
    "final_df = tweets_df[['username', 'text', 'timestamp', 'language_code', 'translated_text', \n",
    "                        'keywords', 'sentence_sentiment', 'extracted_locations', \n",
    "                        'mentioned_users', 'political_inclination']]\n",
    "final_df.to_csv('analyzed_tweets.csv', index=False)\n",
    "print(\"\\nEnriched dataset saved to 'analyzed_tweets.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Dataset:\n",
      "Shape: (33, 9)\n",
      "\n",
      "Users Dataset:\n",
      "Shape: (27, 6)\n",
      "\n",
      "Creating wordcloud...\n",
      "Creating co-occurrence heatmap...\n",
      "Creating user location map...\n",
      "Creating tweet time-series map...\n",
      "Plotting sentiment trend line...\n",
      "Plotting daily tweet count trend...\n",
      "Performing topic modeling...\n",
      "Topic 1: ww2, churchill, did, military, white, winston, don, people, generation, order\n",
      "Topic 2: europe, ww2, right, ww3, need, nazis, ussr, america, don, going\n",
      "Topic 3: ww3, wants, fighting, won, weapons, going, ukraine, people, nation, hasn\n",
      "Topic 4: ww2, world, like, think, need, europe, war, america, saved, don\n",
      "Topic 5: ww2, war, start, world, won, allied, grandfather, allies, order, ussr\n",
      "Creating additional visualizations...\n",
      "\n",
      "Generating AI summary report...\n",
      "Analysis complete! All visualizations and the report have been saved.\n",
      "\n",
      "Enriched dataset saved to 'analyzed_tweets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import language detection and translation libraries\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "# Import spaCy and geopy for location extraction and geocoding\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize geopy Nominatim geocoder and googletrans translator\n",
    "geolocator = Nominatim(user_agent=\"tweet_geocoder\")\n",
    "translator = Translator()\n",
    "\n",
    "# Load datasets\n",
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "\n",
    "# Display basic dataset info\n",
    "print(\"Tweets Dataset:\")\n",
    "print(f\"Shape: {tweets_df.shape}\")\n",
    "print(\"\\nUsers Dataset:\")\n",
    "print(f\"Shape: {users_df.shape}\")\n",
    "\n",
    "# Step 1: Sort tweets by timestamp\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df['timestamp'])\n",
    "tweets_df = tweets_df.sort_values(by='timestamp')\n",
    "\n",
    "# Step 2: Detect language using langdetect\n",
    "def detect_language(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except Exception:\n",
    "            return 'en'\n",
    "    return 'en'\n",
    "\n",
    "tweets_df['language_code'] = tweets_df['text'].apply(detect_language)\n",
    "\n",
    "# Step 3: Translate non-English text to English using googletrans\n",
    "def translate_to_english(row):\n",
    "    if row['language_code'] != 'en' and isinstance(row['text'], str):\n",
    "        try:\n",
    "            translation = translator.translate(row['text'], dest='en')\n",
    "            return translation.text\n",
    "        except Exception:\n",
    "            return row['text']\n",
    "    return row['text']\n",
    "\n",
    "tweets_df['translated_text'] = tweets_df.apply(translate_to_english, axis=1)\n",
    "analysis_text = tweets_df['translated_text'].fillna('')\n",
    "\n",
    "# Step 4: Extract keywords using TF-IDF\n",
    "stop_words = list(stopwords.words('english'))  # convert stopwords to list\n",
    "vectorizer = TfidfVectorizer(max_features=5, stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(analysis_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(tfidf_row, feature_names):\n",
    "    indices = np.argsort(tfidf_row)[::-1]\n",
    "    top_keywords = [(feature_names[i], tfidf_row[i]) for i in indices if tfidf_row[i] > 0]\n",
    "    return top_keywords\n",
    "\n",
    "tweets_df['keywords'] = [get_top_keywords(tfidf_matrix[i].toarray()[0], feature_names) \n",
    "                           for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Step 5: Compute sentiment scores (for each tweet and for keywords)\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "tweets_df['keyword_sentiments'] = tweets_df['keywords'].apply(\n",
    "    lambda kw_list: [(kw, score, get_sentiment(kw)) for kw, score in kw_list] if isinstance(kw_list, list) else []\n",
    ")\n",
    "tweets_df['sentence_sentiment'] = tweets_df['translated_text'].apply(get_sentiment)\n",
    "\n",
    "# Step 6: Extract geographic locations from tweets using spaCy NER and geocode them\n",
    "def extract_locations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    # Extract entities labeled as GPE (Countries, cities) or LOC\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return list(set(locations))  # Return unique locations\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location)\n",
    "        if loc:\n",
    "            return (loc.latitude, loc.longitude, location)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "tweets_df['extracted_locations'] = tweets_df['translated_text'].apply(extract_locations)\n",
    "tweets_df['location_coordinates'] = tweets_df['extracted_locations'].apply(\n",
    "    lambda locs: [get_coordinates(loc) for loc in locs if loc]\n",
    ")\n",
    "tweets_df['location_coordinates'] = tweets_df['location_coordinates'].apply(\n",
    "    lambda coords: [c for c in coords if c is not None]\n",
    ")\n",
    "\n",
    "# Step 7: Extract mentioned users using regex\n",
    "def extract_mentions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return list(set(mentions))\n",
    "\n",
    "tweets_df['mentioned_users'] = tweets_df['text'].apply(extract_mentions)\n",
    "\n",
    "# Step 8: Assess political inclination via a keyword-based approach\n",
    "def assess_political_inclination(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'neutral'\n",
    "    left_keywords = ['democrat', 'liberal', 'progressive', 'left', 'biden', 'harris']\n",
    "    right_keywords = ['republican', 'conservative', 'maga', 'trump', 'right']\n",
    "    text_lower = text.lower()\n",
    "    left_count = sum(1 for kw in left_keywords if kw in text_lower)\n",
    "    right_count = sum(1 for kw in right_keywords if kw in text_lower)\n",
    "    if left_count > right_count:\n",
    "        return 'left-leaning'\n",
    "    elif right_count > left_count:\n",
    "        return 'right-leaning'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "tweets_df['political_inclination'] = tweets_df['translated_text'].apply(assess_political_inclination)\n",
    "\n",
    "# Step 9: Create wordcloud and co-occurrence heatmap of keywords\n",
    "def create_wordcloud(keywords):\n",
    "    word_freq = {}\n",
    "    for tweet_keywords in keywords:\n",
    "        for word, score in tweet_keywords:\n",
    "            word_freq[word] = word_freq.get(word, 0) + score\n",
    "    if word_freq:\n",
    "        wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('wordcloud.png')\n",
    "        plt.close()\n",
    "\n",
    "def create_cooccurrence_heatmap(df):\n",
    "    all_keywords = set()\n",
    "    for kw_list in df['keywords']:\n",
    "        all_keywords.update([kw for kw, _ in kw_list])\n",
    "    all_keywords = list(all_keywords)\n",
    "    \n",
    "    cooccurrence = np.zeros((len(all_keywords), len(all_keywords)))\n",
    "    for kw_list in df['keywords']:\n",
    "        words = [kw for kw, _ in kw_list]\n",
    "        for i, word1 in enumerate(all_keywords):\n",
    "            for j, word2 in enumerate(all_keywords):\n",
    "                if word1 in words and word2 in words and i != j:\n",
    "                    cooccurrence[i, j] += 1\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cooccurrence, xticklabels=all_keywords, yticklabels=all_keywords, cmap='Blues')\n",
    "    plt.title('Keyword Co-occurrence Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cooccurrence_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 10: Plot linear regression trend for tweet sentiment over a numeric sequence\n",
    "def plot_trend_line(df, column, title, filename):\n",
    "    X = np.arange(len(df)).reshape(-1, 1)  # Numeric tweet index\n",
    "    Y = df[column].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, Y, label='Data', alpha=0.5)\n",
    "    plt.plot(X, trend, color='red', label='Trend line')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Tweet Index (Numeric Sequence)\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Step 11: Plot time series for daily tweet count with linear regression trend\n",
    "def plot_tweet_count_trend(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = df_copy['timestamp'].dt.date\n",
    "    tweet_count = df_copy.groupby('date').size().reset_index(name='count')\n",
    "    X = np.arange(len(tweet_count)).reshape(-1, 1)\n",
    "    Y = tweet_count['count'].values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    trend = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(tweet_count['date'], Y, label='Daily Tweet Count', alpha=0.5)\n",
    "    plt.plot(tweet_count['date'], trend, color='red', label='Trend Line')\n",
    "    plt.title('Daily Tweet Count with Trend Line')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tweet Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('daily_tweet_count_trend.png')\n",
    "    plt.close()\n",
    "\n",
    "# Step 12: Create maps\n",
    "# Map One: Map of user locations (using users_df and their \"location\" field)\n",
    "def create_user_location_map(users_df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    # Assumes users_df contains columns 'username' and 'location'\n",
    "    for idx, row in users_df.iterrows():\n",
    "        if pd.notnull(row.get('location')):\n",
    "            try:\n",
    "                loc = geolocator.geocode(row['location'])\n",
    "                if loc:\n",
    "                    folium.Marker(\n",
    "                        location=[loc.latitude, loc.longitude],\n",
    "                        popup=f\"User: {row['username']}<br>Location: {row['location']}\",\n",
    "                        icon=folium.Icon(color='green')\n",
    "                    ).add_to(m)\n",
    "            except Exception:\n",
    "                continue\n",
    "    m.save('user_locations_map.html')\n",
    "    return m\n",
    "\n",
    "# Map Two: Time-series map of tweet-extracted locations (using geocoded locations from tweet texts)\n",
    "def create_tweet_time_series_map(df):\n",
    "    m = folium.Map(location=[20, 0], zoom_start=2)\n",
    "    features = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Use only the geocoded tweet locations (extracted from tweet text)\n",
    "        for coordinate in row['location_coordinates']:\n",
    "            if coordinate:\n",
    "                lat, lon, loc_name = coordinate\n",
    "                time_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                feature = {\n",
    "                    'type': 'Feature',\n",
    "                    'geometry': {\n",
    "                        'type': 'Point',\n",
    "                        'coordinates': [lon, lat]\n",
    "                    },\n",
    "                    'properties': {\n",
    "                        'time': time_str,\n",
    "                        'popup': f\"User: {row['username']}<br>Tweet: {row['text'][:100]}...<br>Location: {loc_name}\"\n",
    "                    }\n",
    "                }\n",
    "                features.append(feature)\n",
    "    if features:\n",
    "        TimestampedGeoJson(\n",
    "            {'type': 'FeatureCollection', 'features': features},\n",
    "            period='PT1H',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=False,\n",
    "            max_speed=1,\n",
    "            loop_button=True,\n",
    "            time_slider_drag_update=True\n",
    "        ).add_to(m)\n",
    "    m.save('tweet_time_series_map.html')\n",
    "    return m\n",
    "\n",
    "# Step 13: Topic modeling using LDA\n",
    "def perform_topic_modeling(texts, n_topics=5):\n",
    "    count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    feature_names = count_vect.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append((f\"Topic {topic_idx+1}\", top_words))\n",
    "    return topics\n",
    "\n",
    "# Step 14: Additional visualizations for top entities\n",
    "def create_top_visualizations(df):\n",
    "    # Top languages\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['language_code'].value_counts().plot(kind='bar')\n",
    "    plt.title('Top Languages')\n",
    "    plt.xlabel('Language Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_languages.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Top mentioned users\n",
    "    all_mentions = []\n",
    "    for mentions in df['mentioned_users']:\n",
    "        all_mentions.extend(mentions)\n",
    "    if all_mentions:\n",
    "        mention_counts = pd.Series(all_mentions).value_counts().head(10)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        mention_counts.plot(kind='bar')\n",
    "        plt.title('Top Mentioned Users')\n",
    "        plt.xlabel('Username')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top_mentions.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Political inclination distribution (pie chart)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['political_inclination'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Political Inclination Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('political_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter plot: Sentiment vs. Political inclination\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {'left-leaning': 'blue', 'right-leaning': 'red', 'neutral': 'green'}\n",
    "    for inclination, color in colors.items():\n",
    "        subset = df[df['political_inclination'] == inclination]\n",
    "        plt.scatter(\n",
    "            subset.index, \n",
    "            subset['sentence_sentiment'],\n",
    "            c=color,\n",
    "            label=inclination,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    plt.title('Sentiment vs Political Inclination')\n",
    "    plt.xlabel('Tweet Index (Time Ordered)')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_vs_politics.png')\n",
    "    plt.close()\n",
    "\n",
    "# ------------------ Execute Analysis Steps ------------------\n",
    "\n",
    "print(\"\\nCreating wordcloud...\")\n",
    "create_wordcloud(tweets_df['keywords'])\n",
    "\n",
    "print(\"Creating co-occurrence heatmap...\")\n",
    "create_cooccurrence_heatmap(tweets_df)\n",
    "\n",
    "print(\"Creating user location map...\")\n",
    "create_user_location_map(users_df)\n",
    "\n",
    "print(\"Creating tweet time-series map...\")\n",
    "create_tweet_time_series_map(tweets_df)\n",
    "\n",
    "print(\"Plotting sentiment trend line...\")\n",
    "plot_trend_line(tweets_df, 'sentence_sentiment', 'Tweet Sentiment Trend', 'sentiment_trend.png')\n",
    "\n",
    "print(\"Plotting daily tweet count trend...\")\n",
    "plot_tweet_count_trend(tweets_df)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "topics = perform_topic_modeling(tweets_df['translated_text'].fillna(''), n_topics=5)\n",
    "for topic_name, top_words in topics:\n",
    "    print(f\"{topic_name}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "create_top_visualizations(tweets_df)\n",
    "\n",
    "# Generate comprehensive AI summary report\n",
    "print(\"\\nGenerating AI summary report...\")\n",
    "\n",
    "total_tweets = len(tweets_df)\n",
    "unique_users = tweets_df['username'].nunique()\n",
    "avg_sentiment = tweets_df['sentence_sentiment'].mean()\n",
    "political_counts = tweets_df['political_inclination'].value_counts()\n",
    "\n",
    "report = f\"\"\"\n",
    "# Twitter Data Analysis Report\n",
    "\n",
    "## Overview\n",
    "- Total Tweets Analyzed: {total_tweets}\n",
    "- Unique Users: {unique_users}\n",
    "- Average Sentiment Score: {avg_sentiment:.2f}\n",
    "\n",
    "## Political Distribution\n",
    "{political_counts.to_string()}\n",
    "\n",
    "## Top Topics\n",
    "\"\"\"\n",
    "\n",
    "for topic_name, top_words in topics:\n",
    "    report += f\"- {topic_name}: {', '.join(top_words)}\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "## Key Findings\n",
    "- Non-English tweets are detected using langdetect and translated to English via googletrans.\n",
    "- Two maps are generated: one showing user locations and one (time-series) showing tweet-extracted geolocations.\n",
    "- Linear regression on tweet index provides a trend line for sentiment, and daily tweet counts are analyzed.\n",
    "- Topic modeling (LDA) identifies distinct themes within the tweets.\n",
    "\n",
    "## Generated Visualizations\n",
    "- Wordcloud of keywords\n",
    "- Keyword co-occurrence heatmap\n",
    "- User locations map\n",
    "- Time-series map of tweet-extracted locations\n",
    "- Sentiment trend line (with linear regression)\n",
    "- Daily tweet count trend\n",
    "- Political inclination distribution and sentiment vs. politics scatter plot\n",
    "\"\"\"\n",
    "\n",
    "with open('twitter_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Analysis complete! All visualizations and the report have been saved.\")\n",
    "\n",
    "# Create a final enriched DataFrame and save it\n",
    "final_df = tweets_df[['username', 'text', 'timestamp', 'language_code', 'translated_text', \n",
    "                        'keywords', 'sentence_sentiment', 'extracted_locations', \n",
    "                        'mentioned_users', 'political_inclination']]\n",
    "final_df.to_csv('analyzed_tweets.csv', index=False)\n",
    "print(\"\\nEnriched dataset saved to 'analyzed_tweets.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33 tweets\n",
      "Identified 76 key terms\n",
      "Main term: 'ww2' with 46 connections\n",
      "Visualization saved to term_cooccurrence_graph.html\n",
      "\n",
      "--- ANALYSIS SUMMARY ---\n",
      "Key themes:\n",
      "- Historical Comparisons: Drawing parallels between WW2 and current geopolitical situations (particularly Ukraine).\n",
      "- Escalation Concerns: Anxiety about the potential for WW3 and the use of nuclear weapons.\n",
      "- Geopolitical Involvement: Discussions regarding the roles of Europe, Russia, USA, and other nations in current conflicts and historical events.\n",
      "- Political Commentary: Mentions of figures like Trump, Putin, and Churchill, and their potential influence on world events.\n",
      "- Ideological and Generational Perspectives: Exploring different viewpoints, potentially related to generational experiences and political ideologies.\n",
      "\n",
      "Summary:\n",
      "This term network centered around 'ww2' reveals a complex interplay between historical reflection and contemporary concerns. The high frequency of terms related to WW3 and nuclear weapons underscores anxieties about potential global conflicts. The presence of terms associated with Ukraine and Russia strongly suggests a focus on the current geopolitical situation and its potential parallels with WW2. The network also includes political commentary and explores varying perspectives, potentially shaped by generational differences and ideological leanings. Overall, the network reflects a discourse that attempts to understand current events through the lens of historical precedent, while also grappling with the potential for escalation and global instability.\n",
      "Analysis saved to tweet_cooccurrence_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import google.generativeai as genai\n",
    "from pyvis.network import Network\n",
    "import webbrowser\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Set your API key\n",
    "genai.configure(api_key=\"AIzaSyBQjwl1U4208zTqqoOvAhjo98ypbCs8Pk4\")\n",
    "\n",
    "# Define the model\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess tweet text to extract meaningful terms.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, and special characters\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stop words (simplified version)\n",
    "    stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'in', 'on', 'at', 'to', 'for', 'with', \n",
    "                 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', \n",
    "                 'does', 'did', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'this', 'that', 'of', \n",
    "                 'from', 'by', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "    \n",
    "    # Only keep tokens that are not stop words and have length > 2\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def extract_key_terms(tweets_df, min_count=2, max_terms=500):\n",
    "    \"\"\"\n",
    "    Extract key terms from all tweets based on frequency.\n",
    "    \"\"\"\n",
    "    all_terms = []\n",
    "    for text in tweets_df['text']:\n",
    "        all_terms.extend(preprocess_text(text))\n",
    "    \n",
    "    # Count term frequencies\n",
    "    term_counts = Counter(all_terms)\n",
    "    \n",
    "    # Filter by minimum count\n",
    "    key_terms = {term: count for term, count in term_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Limit to max number of terms (take most frequent)\n",
    "    if len(key_terms) > max_terms:\n",
    "        key_terms = dict(sorted(key_terms.items(), key=lambda x: x[1], reverse=True)[:max_terms])\n",
    "    \n",
    "    return key_terms\n",
    "\n",
    "def visualize_term_cooccurrence(df, min_term_count=2, max_terms=200, min_edge_weight=2, output_file=\"term_cooccurrence_graph.html\"):\n",
    "    \"\"\"\n",
    "    Creates and visualizes a term co-occurrence network from tweets.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing a 'text' column with tweet content.\n",
    "    min_term_count (int): Minimum frequency for a term to be included.\n",
    "    max_terms (int): Maximum number of terms to include.\n",
    "    min_edge_weight (int): Minimum co-occurrence weight for an edge to be included.\n",
    "    output_file (str): Name of the output HTML file.\n",
    "    \"\"\"\n",
    "    # Extract key terms\n",
    "    key_terms = extract_key_terms(df, min_count=min_term_count, max_terms=max_terms)\n",
    "    print(f\"Identified {len(key_terms)} key terms\")\n",
    "    \n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Process each tweet\n",
    "    for _, row in df.iterrows():\n",
    "        if not isinstance(row['text'], str):\n",
    "            continue\n",
    "        \n",
    "        # Extract terms from tweet\n",
    "        tweet_terms = preprocess_text(row['text'])\n",
    "        \n",
    "        # Filter to only include key terms\n",
    "        tweet_terms = [term for term in tweet_terms if term in key_terms]\n",
    "        \n",
    "        # Add co-occurrences to graph\n",
    "        for i in range(len(tweet_terms)):\n",
    "            for j in range(i+1, len(tweet_terms)):\n",
    "                term1, term2 = tweet_terms[i], tweet_terms[j]\n",
    "                \n",
    "                # Add nodes if they don't exist\n",
    "                if not G.has_node(term1):\n",
    "                    G.add_node(term1, count=key_terms[term1])\n",
    "                if not G.has_node(term2):\n",
    "                    G.add_node(term2, count=key_terms[term2])\n",
    "                \n",
    "                # Add or update edge weight\n",
    "                if G.has_edge(term1, term2):\n",
    "                    G[term1][term2]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(term1, term2, weight=1)\n",
    "    \n",
    "    # Remove edges with weight below threshold\n",
    "    edges_to_remove = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_edge_weight]\n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "    \n",
    "    # Remove isolated nodes\n",
    "    isolated_nodes = list(nx.isolates(G))\n",
    "    G.remove_nodes_from(isolated_nodes)\n",
    "    \n",
    "    if len(G.nodes()) == 0:\n",
    "        print(\"No significant co-occurrences found with current parameters.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Identify the main node (highest degree)\n",
    "    main_node, main_degree = max(G.degree(), key=lambda x: x[1])\n",
    "    print(f\"Main term: '{main_node}' with {main_degree} connections\")\n",
    "    \n",
    "    # Create JSON variables for nodes and their degree counts\n",
    "    nodes_count_json = json.dumps({node: data.get('count', 0) for node, data in G.nodes(data=True)}, indent=4)\n",
    "    \n",
    "    # Get loosely linked nodes from the main node\n",
    "    loose_threshold = 2\n",
    "    loosely_linked = [n for n in G.neighbors(main_node) if G.degree(n) <= loose_threshold]\n",
    "    loosely_linked_json = json.dumps({node: G.degree(node) for node in loosely_linked}, indent=4)\n",
    "    \n",
    "    # Create Pyvis Network\n",
    "    net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\", notebook=False, select_menu=True)\n",
    "    net.barnes_hut(gravity=-20000, central_gravity=0.3, spring_length=250, spring_strength=0.001)\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for node, attr in G.nodes(data=True):\n",
    "        count = attr.get('count', 0)\n",
    "        net.add_node(node, title=f\"Term: {node}<br>Frequency: {count}<br>Connections: {G.degree(node)}\", value=count)\n",
    "    \n",
    "    # Add edges with weights\n",
    "    for u, v, attr in G.edges(data=True):\n",
    "        weight = attr.get('weight', 1)\n",
    "        net.add_edge(u, v, value=weight, title=f\"Co-occurrence: {weight}\")\n",
    "    \n",
    "    # Set node sizes based on term frequency\n",
    "    max_count = max([data.get('count', 1) for _, data in G.nodes(data=True)])\n",
    "    min_size, max_size = 10, 50\n",
    "    \n",
    "    for node in net.nodes:\n",
    "        node_id = node.get('id')\n",
    "        if node_id in G.nodes:\n",
    "            count = G.nodes[node_id].get('count', 1)\n",
    "            size = min_size + (count / max_count) * (max_size - min_size)\n",
    "            node.update({\"size\": size})\n",
    "    \n",
    "    # Set styling options\n",
    "    net.set_options(\"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"scaling\": {\"min\": 10, \"max\": 50},\n",
    "        \"color\": {\"border\": \"#2B7CE9\", \"background\": \"#97C2FC\"},\n",
    "        \"font\": {\"size\": 16, \"face\": \"arial\", \"color\": \"#343434\", \"align\": \"center\"}\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\"color\": \"#848484\", \"inherit\": false},\n",
    "        \"smooth\": {\"enabled\": true, \"type\": \"dynamic\"},\n",
    "        \"width\": 0.5\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"barnesHut\": {\"gravitationalConstant\": -20000, \"centralGravity\": 0.3, \"springLength\": 250, \"springConstant\": 0.001},\n",
    "        \"minVelocity\": 0.75\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    # Generate AI analysis of the network\n",
    "    prompt = f'''\n",
    "    I have created a graph showing how terms co-occur in tweets from Twitter data.\n",
    "    The main term is '{main_node}' which has {main_degree} connections.\n",
    "    \n",
    "    These are my terms and their frequencies:\n",
    "    {nodes_count_json}\n",
    "    \n",
    "    These are terms loosely linked to my main term:\n",
    "    {loosely_linked_json}\n",
    "    \n",
    "    Give me an analysis of the key topics and themes in this conversation network.\n",
    "    Identify any clusters of terms that might represent distinct narratives or topics.\n",
    "    \n",
    "    Output in JSON format with:\n",
    "    - key_themes: list of main themes/topics identified\n",
    "    - topic_clusters: object with cluster names as keys and relevant terms as values\n",
    "    - interesting_insights: list of 3-5 observations about the data\n",
    "    - summary_report: brief analysis of what this term network reveals\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        analysis_text = response.text\n",
    "        \n",
    "        # Try to clean up the response to get valid JSON\n",
    "        if '```json' in analysis_text:\n",
    "            analysis_text = analysis_text.split('```json')[1].split('```')[0].strip()\n",
    "        elif '```' in analysis_text:\n",
    "            analysis_text = analysis_text.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        analysis_json = json.loads(analysis_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating AI analysis: {str(e)}\")\n",
    "        analysis_json = {\n",
    "            \"key_themes\": [\"Analysis not available\"],\n",
    "            \"summary_report\": \"Could not generate analysis due to an error.\"\n",
    "        }\n",
    "    \n",
    "    # Save and open visualization\n",
    "    net.write_html(output_file)\n",
    "    webbrowser.open(output_file)\n",
    "    print(f\"Visualization saved to {output_file}\")\n",
    "    \n",
    "    return G, analysis_json\n",
    "\n",
    "def analyze_tweets_cooccurrence(tweets_file, min_term_count=2, max_terms=200, min_edge_weight=2):\n",
    "    \"\"\"\n",
    "    Main function to load tweets and analyze co-occurrences.\n",
    "    \"\"\"\n",
    "    # Load tweets\n",
    "    df = pd.read_csv(tweets_file)\n",
    "    print(f\"Loaded {len(df)} tweets\")\n",
    "    \n",
    "    # Run visualization and analysis\n",
    "    graph, analysis = visualize_term_cooccurrence(\n",
    "        df, \n",
    "        min_term_count=min_term_count,\n",
    "        max_terms=max_terms,\n",
    "        min_edge_weight=min_edge_weight\n",
    "    )\n",
    "    \n",
    "    if analysis:\n",
    "        print(\"\\n--- ANALYSIS SUMMARY ---\")\n",
    "        print(\"Key themes:\")\n",
    "        for theme in analysis.get('key_themes', []):\n",
    "            print(f\"- {theme}\")\n",
    "            \n",
    "        print(\"\\nSummary:\")\n",
    "        print(analysis.get('summary_report', 'No summary available'))\n",
    "    \n",
    "    return graph, analysis\n",
    "\n",
    "# Run the analysis on the tweets\n",
    "if __name__ == \"__main__\":\n",
    "    graph, analysis = analyze_tweets_cooccurrence(\n",
    "        \"tweets.csv\",\n",
    "        min_term_count=2,  # Terms must appear at least twice\n",
    "        max_terms=200,     # Include up to 200 terms\n",
    "        min_edge_weight=2  # Terms must co-occur at least twice\n",
    "    )\n",
    "    \n",
    "    # Save analysis to file\n",
    "    if analysis:\n",
    "        with open(\"tweet_cooccurrence_analysis.json\", \"w\") as f:\n",
    "            json.dump(analysis, indent=4, fp=f)\n",
    "        print(\"Analysis saved to tweet_cooccurrence_analysis.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
